\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{rotating}

\usepackage{amsmath}

\title{Project documentation:\\
       \textbf{HMC with OpenCL for hybrid manycore systems}
}

\author{Christopher Pinke \and Lars Zeidlewicz}

\date{\today}

\bibliographystyle{unsrt}

\newcommand{\file}[1]{\texttt{#1}}
\newcommand{\define}[1]{\texttt{#1}}
\newcommand{\ctype}[1]{\texttt{#1}}
\newcommand{\function}[1]{\texttt{#1}}
\newcommand{\url}[1]{\texttt{#1}}

\begin{document}
\maketitle
\begin{abstract}
We want to write a complete HMC simulation programme with Wilson-type quarks (twisted mass action) using OpenCL for a hybrid structure consisting of CPUs and GPUs.
\end{abstract}

\tableofcontents

\newpage

\section{General thoughts -- structure}
This will be a lot of work\ldots

Basically, the programme splits into two main parts: host-code and device-code. Host-code is standard C++ and provides the OpenCL-interface and master code for the OpenCL kernels. Furthermore, in the end, it is planned to have a completely working CPU-HMC, too. Since it is meant to run on a single node, the use of OpenMP is intended. The CPU-HMC might be needed to implement the hybrid approach in a later step.

The device code is the collection of all OpenCL-kernels. This should finally constitute a complete HMC. For now, we target at a heatbath implementation. The pure gauge heatbath will provide a good testing ground for a) benchmarks (and performance optimisation), b) the hybrid strategy. At this stage, we also want to learn what the optimal communication between host and device is, i.\,e. when do we need to transfer what information.

Data types for the C++ host and OpenCL device codes are not the same. Therefore, the C++ code also needs to provide transfer functions. For the gaugefield and SU($N_c$) types this should be finished. For the spinor part, since its not needed yet, no OpenCL types have been defined so far.

On the host, there are plaquette and Polyakov functions that have been tested against existing configuration files. It is an open question whether we want to have those measurements on the device, too. The alternative would be to measure the gauge observables only when the gaugefield is transferred back to the host.

ILDG format configurations (as from Carsten Urbach's code \cite{Jansen:2009xp}) can be read. We still need the according functions to write a config to a file.

\section{Algorithm}
\input{algorithm}

\section{Options and I/O}
\input{inputoutput}

\section{Functions and constants}
\input{programcomponents}

\section{Hybrid strategy}
\input{hybridstrategy}

\section{Some readings}
\subsection{ILDG}
Here one can find infos about ILDG: \url{http://ildg.sasr.edu.au/Plone}

\subsection{LQCD using OpenCL}
To get the OpenCL specifications visit \url{http://www.khronos.org/opencl}.

\begin{itemize}
\item With OpenGL: Egri et al., Lattice QCD as a video game \cite{Egri:2006zm} (classic)
\item Demchik and Strelchenko, SU(2) Monte Carlo \cite{Demchik:2009ni}
\item Demchik, Random Numbers on GPUs with OpenCL \cite{Demchik:2010}
\end{itemize}

\subsection{LQCD using CUDA}
To get general CUDA informations visit \url{http://www.nvidia.com/object/cuda\_home\_new.html}.

\begin{itemize}
\item Cardoso and Bicudo, Lattice SU(2) on GPUs \cite{Cardoso:2010me}
\item Clark et al., Solving Lattice QCD systems of equations using mixed precision solvers on GPUs \cite{Clark:2009wm}
\item Clark, QCD on GPUs: cost effective supercomputing \cite{Clark:2009qp}
\item Hayakawa et al., Improving many flavor QCD simulations using multiple GPUs \cite{Hayakawa:2010gm}
\item TWQCD Collaboration: TWQCD's dynamical DWF project \cite{Chiu:2009wh}
\item Bonati, Cossu, D'Elia, Di Giacomo, Staggered fermions simulations on GPUs~\cite{Bonati:2010qu}
\item Kim and Lee, Multi GPU Performance of Conjugate Gradient Algorithm with Staggered Fermions \cite{Kim:2010br}
\item Walk, Wittig, Dranischnikow, Schomer,Implementation of the Neuberger-Dirac operator on GPUs \cite{Walk:2010ut}
\item Karimi et al., High-Performance Physics Simulations Using Multi-Core CPUs and GPGPUs in a Volunteer Computing Context \cite{DBLP:journals/corr/abs-1004-0023}, A Performance Comparison of CUDA and OpenCL \cite{DBLP:journals/corr/abs-1005-2581}
\item Osaki and Ishikawa, Domain Decomposition on GPU: \cite{osaki:2010}
\end{itemize}


\bibliography{literature}


\end{document}