/** @file
 * A simple microbenchmark to check device bandwidth for different
 * numbers of threada and streaming.
 *
 * Copyright (c) 2011-2013 Matthias Bach
 * Copyright (c) 2014 Christopher Pinke
 * Copyright (c) 2015 Francesca Cuteri
 * Copyright (c) 2018 Alessandro Sciarra
 *
 * This file is part of CL2QCD.
 *
 * CL2QCD is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * CL2QCD is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with CL2QCD. If not, see <http://www.gnu.org/licenses/>.
 */

#include "../common_header_files/types_fermions.hpp"
#include "../executables/exceptions.hpp"
#include "../hardware/code/gaugefield.hpp"
#include "../hardware/device.hpp"
#include "../hardware/system.hpp"
#include "../host_functionality/logger.hpp"
#include "../klepsydra/klepsydra.hpp"

#include <boost/program_options.hpp>
#include <stdexcept>
#include <string>

namespace po = boost::program_options;

/**
 * Selector type for the base type of the copy operations.
 */
enum copyType {
    type_invalid,
    type_float,
    type_su3,
    type_su3SOA,
    type_su3SOcplxA,
    type_spinor,
    type_spinorSOA,
    type_spinorSOcplxA,
    type_spinorLocal,
    type_spinorSOApy
};

size_t getTypeSize(copyType type);

class Device : public hardware::code::Opencl_Module {
  private:
    cl_kernel floatKernel;
    cl_kernel su3Kernel;
    cl_kernel su3SOAKernel;
    cl_kernel su3SOcplxAKernel;
    cl_kernel spinorKernel;
    cl_kernel spinorSOAKernel;
    cl_kernel spinorSOcplxAKernel;
    cl_kernel spinorLocalKernel;
    cl_kernel spinorSOApyKernel;

    template<typename T>
    void runKernel(size_t groups, cl_ulong threads_per_group, cl_ulong elems, cl_kernel kernel,
                   const hardware::buffers::Plain<cl_char>* in, const hardware::buffers::Plain<cl_char>* out);

    void fill_kernels();
    void clear_kernels();

  protected:
    virtual size_t get_read_write_size(const std::string&) const { return 0; };
    virtual uint64_t get_flop_size(const std::string&) const { return 0; };

  public:
    Device(const hardware::code::OpenClKernelParametersInterface& params, hardware::Device* device)
        : Opencl_Module(params, device)
    {
        fill_kernels();
    };
    virtual ~Device() { clear_kernels(); };

    void runKernel(copyType copy_type, size_t groups, cl_ulong threads_per_group, cl_ulong elems,
                   const hardware::buffers::Plain<cl_char>* in, const hardware::buffers::Plain<cl_char>* out);
};

class Test {
  public:
    Test(const hardware::System& system, size_t maxMemSize)
        : maxMemSize(maxMemSize), device(system.get_devices().at(0)), code(*(system.getOpenClParameters()), device)
    {
        fill_buffers();
    };

    void runKernel(copyType copy_type, size_t groups, cl_ulong threads_per_group, cl_ulong elems);

    ~Test();

  private:
    void verify(hmc_complex, hmc_complex);
    void fill_buffers();
    const hardware::buffers::Plain<cl_char>*in, *out;
    size_t maxMemSize;
    hardware::Device* const device;
    Device code;
};

int main(int argc, char** argv)
{
    po::options_description desc("Allowed options");
    // clang-format off
    desc.add_options()
    ("help,h", "Produce this help message")
    ("elements,e", po::value<cl_ulong>()->default_value(100000), "How many elements to use.") // conflicts with single
    ("stepelements,se", po::value<cl_ulong>(), "Step size for element count sweeping")
    ("threads,t", po::value<cl_ulong>()->default_value(64), "The number of threads to use per groups")
    ("groups,g", po::value<cl_ulong>(), "Specify a fixed number of groups.")
    ("stepthreads,st", po::value<cl_ulong>()->default_value(0), "Step size for thread per group sweeping")
    ("single", "Copy only a single element per thread")
    ("type,d", po::value<std::string>()->default_value("float"), "The data type to copy");
    // clang-format on

    po::variables_map vm;
    po::store(po::parse_command_line(argc, argv, desc), vm);
    po::notify(vm);

    // Check if help is needed
    if (vm.count("help")) {
        std::cout << desc << '\n';
        return 0;
    }

    // parse type
    std::map<std::string, copyType> type_map;
    type_map["float"]         = type_float;
    type_map["su3"]           = type_su3;
    type_map["su3SOA"]        = type_su3SOA;
    type_map["su3SOcplxA"]    = type_su3SOcplxA;
    type_map["spinor"]        = type_spinor;
    type_map["spinorSOA"]     = type_spinorSOA;
    type_map["spinorSOApy"]   = type_spinorSOApy;
    type_map["spinorSOcplxA"] = type_spinorSOcplxA;
    type_map["spinorLocal"]   = type_spinorLocal;

    const copyType copy_type = type_map[vm["type"].as<std::string>()];
    if (!copy_type) {
        logger.error() << "Please select one of the following types: float(default), su3, su3SOA, su3SOcplxA, spinor, "
                          "spinorSOA, spinorSOApy, spinorSOcplxA, spinorLocal";
        return 1;
    }
    logger.info() << "Using " << vm["type"].as<std::string>() << " as load/store datatype";

    if (vm.count("single") && vm.count("elements")) {
        logger.error() << "You can either use one element per thread or define the global number of elements";
    }

    meta::Inputparameters params(0, 0);
    hardware::System system(params);

    if (vm.count("stepelements")) {
        logger.info() << "Sweeping element count for fixed thread count.";
        cl_ulong max_elements  = vm["elements"].as<cl_ulong>();
        cl_ulong step_elements = vm["stepelements"].as<cl_ulong>();
        cl_ulong threads       = vm["threads"].as<cl_ulong>();
        cl_ulong groups;
        if (vm.count("groups")) {
            groups = vm["groups"].as<cl_ulong>();
        } else {
            groups = 20 * 8;  // taken out of the air and tuned for Cypress
        }
        if (vm.count("single")) {
            logger.fatal() << "Single element per thread mode has not been implemented in element count sweeping mod";
        } else {
            Test dev(system, max_elements * getTypeSize(copy_type));
            size_t elements = 1;
            dev.runKernel(copy_type, groups, threads, elements);
            for (elements = step_elements; elements <= max_elements; elements += step_elements) {
                dev.runKernel(copy_type, groups, threads, elements);
            }
        }

    } else if (vm.count("groups")) {
        cl_ulong max_threads  = vm["threads"].as<cl_ulong>();
        cl_ulong groups       = vm["groups"].as<cl_ulong>();
        cl_ulong step_threads = vm["stepthreads"].as<cl_ulong>();
        cl_ulong min_threads;
        if (step_threads) {
            logger.info() << "Scanning number active threads per group required for maximum memory throughput";
            min_threads = step_threads;
        } else {
            min_threads  = max_threads;
            step_threads = max_threads + 1;
        }
        if (vm.count("single")) {
            logger.info() << "Using a single element per thread";
            Test dev(system, groups * max_threads * getTypeSize(copy_type));
            if (step_threads <= max_threads) {
                size_t threads = 1;
                dev.runKernel(copy_type, groups, threads, groups * threads);
            }
            for (size_t threads = min_threads; threads <= max_threads; threads += step_threads) {
                dev.runKernel(copy_type, groups, threads, groups * threads);
            }
        } else {
            const cl_ulong elems = vm["elements"].as<cl_ulong>();
            logger.info() << "Keeping number of elements fixed at " << elems;
            Test dev(system, elems * getTypeSize(copy_type));
            if (step_threads <= max_threads) {
                size_t threads = 1;
                dev.runKernel(copy_type, groups, threads, groups * threads);
            }
            for (size_t threads = min_threads; threads <= max_threads; threads += step_threads) {
                dev.runKernel(copy_type, groups, threads, elems);
            }
        }
    } else {
        logger.info() << "Scanning number of wavefronts required for maximum memory throughput";
        cl_ulong threads    = vm["threads"].as<cl_ulong>();
        cl_ulong max_groups = 20 * 8;  // the value comes out of nowhere and is tuned for Cypress

        if (vm.count("single")) {
            logger.info() << "Using a single element per thread";
            Test dev(system, max_groups * threads * getTypeSize(copy_type));
            for (size_t groups = 1; groups <= max_groups; ++groups) {
                dev.runKernel(copy_type, groups, threads, groups * threads);
            }
        } else {
            const cl_ulong elems = vm["elements"].as<cl_ulong>();
            logger.info() << "Keeping number of elements fixed at " << elems;
            Test dev(system, elems * getTypeSize(copy_type));
            for (size_t groups = 1; groups <= max_groups; ++groups) {
                dev.runKernel(copy_type, groups, threads, elems);
            }
        }
    }

    logger.info() << "Done";

    return 0;
}

void Test::fill_buffers()
{
    // don't invoke parent function as we don't require the original buffers

    // kernels might stride, always overallocate memory
    // e.g. spinor might pad up to 24 * 8 KiB = 128 KiB -> 1 MiB should be save
    size_t pad_buf      = 1024 * 1024;
    size_t allocMemSize = maxMemSize + pad_buf;

    logger.info() << "Allocating buffers of " << allocMemSize << " bytes.";

    in  = new hardware::buffers::Plain<cl_char>(allocMemSize, device);
    out = new hardware::buffers::Plain<cl_char>(allocMemSize, device);
}

void Device::fill_kernels()
{
    ClSourcePackage basic_opencl_code = get_device()->getGaugefieldCode()->get_sources();
    floatKernel                       = createKernel("copyFloat") << basic_opencl_code << "types_fermions.hpp"
                                            << "../microbenchmarks/bandwidth.cl";
    su3Kernel = createKernel("copySU3") << basic_opencl_code << "types_fermions.hpp"
                                        << "../microbenchmarks/bandwidth.cl";
    su3SOAKernel = createKernel("copySU3SOA") << basic_opencl_code << "types_fermions.hpp"
                                              << "../microbenchmarks/bandwidth.cl";
    su3SOcplxAKernel = createKernel("copySU3SOcplxA") << basic_opencl_code << "types_fermions.hpp"
                                                      << "../microbenchmarks/bandwidth.cl";
    spinorKernel = createKernel("copySpinor") << basic_opencl_code << "types_fermions.hpp"
                                              << "../microbenchmarks/bandwidth.cl";
    spinorSOAKernel = createKernel("copySpinorSOA") << basic_opencl_code << "types_fermions.hpp"
                                                    << "../microbenchmarks/bandwidth.cl";
    spinorSOApyKernel = createKernel("copyDpSpinorFullestSOARestricted")
                        << basic_opencl_code << "types_fermions.hpp"
                        << "../microbenchmarks/bandwidth_spinorSOApy.cl";
    spinorSOcplxAKernel = createKernel("copySpinorSOcplxA") << basic_opencl_code << "types_fermions.hpp"
                                                            << "../microbenchmarks/bandwidth.cl";
    spinorLocalKernel = createKernel("copySpinorLocal") << basic_opencl_code << "types_fermions.hpp"
                                                        << "../microbenchmarks/bandwidth.cl";
}

Test::~Test()
{
    // don't invoke parent function as we don't require the original buffers
    delete in;
    delete out;
}

void Device::clear_kernels()
{
    clReleaseKernel(floatKernel);
    clReleaseKernel(su3Kernel);
    clReleaseKernel(su3SOAKernel);
    clReleaseKernel(su3SOcplxAKernel);
    clReleaseKernel(spinorKernel);
    clReleaseKernel(spinorSOAKernel);
    clReleaseKernel(spinorSOApyKernel);
    clReleaseKernel(spinorSOcplxAKernel);
    clReleaseKernel(spinorLocalKernel);
}

void Device::runKernel(copyType copy_type, size_t groups, cl_ulong threads_per_group, cl_ulong elems,
                       const hardware::buffers::Plain<cl_char>* in, const hardware::buffers::Plain<cl_char>* out)
{
    switch (copy_type) {
        case type_float:
            runKernel<hmc_float>(groups, threads_per_group, elems, floatKernel, in, out);
            return;
        case type_su3:
            runKernel<Matrixsu3>(groups, threads_per_group, elems, su3Kernel, in, out);
            return;
        case type_su3SOA:
            runKernel<Matrixsu3>(groups, threads_per_group, elems, su3SOAKernel, in, out);
            return;
        case type_su3SOcplxA:
            runKernel<Matrixsu3>(groups, threads_per_group, elems, su3SOcplxAKernel, in, out);
            return;
        case type_spinor:
            runKernel<spinor>(groups, threads_per_group, elems, spinorKernel, in, out);
            return;
        case type_spinorSOA:
            runKernel<spinor>(groups, threads_per_group, elems, spinorSOAKernel, in, out);
            return;
        case type_spinorSOApy:
            runKernel<spinor>(groups, threads_per_group, elems, spinorSOApyKernel, in, out);
            return;
        case type_spinorSOcplxA:
            runKernel<spinor>(groups, threads_per_group, elems, spinorSOcplxAKernel, in, out);
            return;
        case type_spinorLocal:
            runKernel<spinor>(groups, threads_per_group, elems, spinorLocalKernel, in, out);
            return;
        default:
            throw std::invalid_argument("runKernel has not been implemented for this type");
    }
}

template<typename T>
void Device::runKernel(size_t groups, cl_ulong threads_per_group, cl_ulong elems, cl_kernel kernel,
                       const hardware::buffers::Plain<cl_char>* in, const hardware::buffers::Plain<cl_char>* out)
{
    cl_int err = CL_SUCCESS;

    // TODO adjust threads_per_group for kernel invocation to proper size but keep requested value for kernel arg
    size_t local_threads = threads_per_group;
    size_t total_threads = groups * local_threads;

    err = clSetKernelArg(kernel, 0, sizeof(cl_mem), out->get_cl_buffer());
    if (err) {
        logger.fatal() << "Failed to set kernel argument: " << err;
        throw Opencl_Error(err);
    }
    err = clSetKernelArg(kernel, 1, sizeof(cl_mem), in->get_cl_buffer());
    if (err) {
        logger.fatal() << "Failed to set kernel argument: " << err;
        throw Opencl_Error(err);
    }
    err = clSetKernelArg(kernel, 2, sizeof(cl_ulong), &elems);
    if (err) {
        logger.fatal() << "Failed to set kernel argument: " << err;
        throw Opencl_Error(err);
    }
    err = clSetKernelArg(kernel, 3, sizeof(cl_ulong), &threads_per_group);
    if (err) {
        logger.fatal() << "Failed to set kernel argument: " << err;
        throw Opencl_Error(err);
    }

    size_t num_meas = 10;

    const hardware::Device* dev = get_device();
    dev->enqueue_kernel(kernel, total_threads, local_threads);
    dev->synchronize();
    klepsydra::Monotonic timer;
    for (size_t i = 0; i < num_meas; ++i)
        dev->enqueue_kernel(kernel, total_threads, local_threads);
    dev->synchronize();
    int64_t kernelTime = timer.getTime() / num_meas;
    if (err) {
        logger.fatal() << "Failed to execute kernel: " << err;
        throw Opencl_Error(err);
    }

    cl_ulong eventTime = dev->getProfilingData(kernel).get_total_time() / num_meas;

    // format is: #groups #threads per group #elements #copied memory in bytes #copy time in mus #bandwidth in megabytes
    // FIXME sizeof can give broken results in case of aligned types (gross size not equal to net content size)
    std::cout << groups * threads_per_group << ' ' << groups << ' ' << threads_per_group << ' ' << elems << ' '
              << elems * sizeof(T) << ' ' << kernelTime << ' ' << (2 * elems * sizeof(T) / eventTime) << ' '
              << eventTime << std::endl;
}

void Test::runKernel(copyType copy_type, size_t groups, cl_ulong threads_per_group, cl_ulong elems)
{
    code.runKernel(copy_type, groups, threads_per_group, elems, in, out);
}

size_t getTypeSize(copyType type)
{
    switch (type) {
        case type_float:
            return sizeof(hmc_float);
        case type_su3:
        case type_su3SOA:
        case type_su3SOcplxA:
            return sizeof(Matrixsu3);
        case type_spinor:
        case type_spinorSOcplxA:
        case type_spinorSOA:
        case type_spinorSOApy:
        case type_spinorLocal:
            return sizeof(spinor);
        default:
            throw std::invalid_argument("getTypeSize has not been implemented for this type.");
    }
}
